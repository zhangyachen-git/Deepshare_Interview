一、k均值聚类

支持向量机，逻辑回归，决策树等经典的机器学习算法主要用于分类问题，即根据一些已经给定的类别的样本，训练某种分类器，使得他能够对类别未知的样本进行分类。

与分类问题不同，聚类是事先并不知道任何样本标签的情况下，通过数据之间的内在关系吧样本划分维若干类别，使得同类别样本之间相似度高，不同类别样本之间的相似度低。

分类问题属于监督学习范畴，而聚类则属于非监督学习。k均值聚类（K-means-Clustering）是最基础的和最常用的聚类算法。他的基本思想是，通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。特别的代价函数可以定义维各个样本距离所属簇中心点的误差平方和。

 
二、简述K均值聚类算法的具体步骤.

K均值聚类的核心目标是将给定的数据集划分成K个簇，并给出每个数据对应的簇中心点。算法的具体步骤描述如下：

1.数据预处理，如归一化，离群点处理等。
2.随机选取K个簇中心，记为u1,u2.......uk,
3.定义代价函数，J（c,u)min minE||x-u||平方。（公式不对）
4.令t=0,1,2...为迭代步数，重复下面过程直到J收敛
对于每一个样本x，将其分配到距离最近的簇
对于每一个类簇K，重新计算该类簇的中心

K均值聚类算法在迭代时，假设当前J没有达到最小值，那么首先固定簇中心{uk},调整每个样本x的所属的类别c来让J函数减少。
然后在固定{ci}，调整簇中心{uk}使J减少。这2个过程交替循环，J单调递减，当J递减到最小值时，{uk}和{ci}也同时收敛。

三、K均值算法的优缺点是什么？如何对其进行调优？

K均值算法有一些缺点，例如受初值和离群点的影响，每次的结果不稳定，结果通常不是全局最优而是局部最优解，无法很好的解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍），不太适用于离散分裂等。

但是K均值聚类算法的优点主要体现在：对于大数据集。K均值聚类算法相对是可伸缩和高效的，他的计算复杂度是O（NKt)接近与线性，其中N是数据对象的数目，K是聚类的簇数，t是迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到局部最优已经可以满足聚类的需求。
四、如何改进？
五、如何证明其收敛性？
