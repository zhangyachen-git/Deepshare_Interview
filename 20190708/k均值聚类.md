一、k均值聚类原理

K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。


二、简述K均值聚类算法的具体步骤.

1 随机选取k个中心点

2 遍历所有数据，将每个数据划分到最近的中心点中

3 计算每个聚类的平均值，并作为新的中心点

4 重复2-3，直到这k个中线点不再变化（收敛了），或执行了足够多的迭代

时间复杂度：O(I*n*k*m)

空间复杂度：O(n*m)

其中m为每个元素字段个数，n为数据量，I为迭代个数。一般I,k,m均可认为是常量，所以时间和空间复杂度可以简化为O(n)，即线性的。

三、K均值算法的优缺点是什么？

优点 
1）原理比较简单，实现也是很容易，收敛速度快。 
2）聚类效果较优。 
3）算法的可解释度比较强。 
4）主要需要调参的参数仅仅是簇数k。

缺点 
1）K值的选取不好把握 
2）对于不是凸的数据集比较难收敛 
3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。 
4） 采用迭代方法，得到的结果只是局部最优。 
5） 对噪音和异常点比较的敏感 

四、如何改进？

对于离群点和孤立点敏感改进1

首先针对（1），对于离群点和孤立点敏感，如何解决？笔者在前面的一篇博客中，提到过离群点检测的LOF算法，通过去除离群点后再聚类，可以减少离群点和孤立点对于聚类效果的影响。

k值选择改进2

k值的选择问题，在安徽大学李芳的硕士论文中提到了k-Means算法的k值自适应优化方法。下面将针对该方法进行总结。 
首先该算法针对K-means算法的以下主要缺点进行了改进： 
1）必须首先给出k（要生成的簇的数目），k值很难选择。事先并不知道给定的数据应该被分成什么类别才是最优的。 
2)初始聚类中心的选择是K-means的一个问题。 
李芳设计的算法思路是这样的：可以通过在一开始给定一个适合的数值给k，通过一次K-means算法得到一次聚类中心。对于得到的聚类中心，根据得到的k个聚类的距离情况，合并距离最近的类，因此聚类中心数减小，当将其用于下次聚类时，相应的聚类数目也减小了，最终得到合适数目的聚类数。可以通过一个评判值E来确定聚类数得到一个合适的位置停下来，而不继续合并聚类中心。重复上述循环，直至评判函数收敛为止，最终得到较优聚类数的聚类结果。

初始聚类中心的选择改进3

对初始聚类中心的选择的优化。一句话概括为：选择批次距离尽可能远的K个点。

只能发现球状簇改进4

只能获取球状簇的根本原因在于，距离度量的方式。在李荟娆的硕士论文K_means聚类方法的改进及其应用中提到了基于2种测度的改进，改进后，可以去发现非负、类椭圆形的数据。但是对于这一改进，个人认为，并没有很好的解决K-means在这一缺点的问题，如果数据集中有不规则的数据，往往通过基于密度的聚类算法更加适合，比如DESCAN算法。

五、如何对其进行调优？

**K-Means类主要参数** 
KMeans类的主要参数有： 
1) n_clusters: 即k值，一般需要多试一些值以获得较好的聚类效果。k值好坏的评估标准在下面会讲。 
2）max_iter： 最大的迭代次数，一般如果是凸数据集的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。 
3）n_init：用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。 
4）init： 即初始值选择的方式，可以为完全随机选择’random’,优化过的’k-means++’或者自己指定初始化的k个质心。一般建议使用默认的’k-means++’。 
5）algorithm：有“auto”, “full” or “elkan”三种选择。”full”就是我们传统的K-Means算法， “elkan”是（机器学习(25)之K-Means聚类算法详解）原理篇讲的elkan K-Means算法。默认的”auto”则会根据数据值是否是稀疏的，来决定如何选择”full”和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是”full”。一般来说建议直接用默认的”auto”

**MiniBatchKMeans类主要参数**

MiniBatchKMeans类的主要参数比KMeans类稍多，主要有： 
1) n_clusters: 即k值，和KMeans类的n_clusters意义一样。 
2）max_iter：最大的迭代次数， 和KMeans类的max_iter意义一样。 
3）n_init：用不同的初始化质心运行算法的次数。这里和KMeans类意义稍有不同，KMeans类里的n_init是用同样的训练集数据来跑不同的初始化质心从而运行算法。而MiniBatchKMeans类的n_init则是每次用不一样的采样数据集来跑不同的初始化质心运行算法。 
4）batch_size：即用来跑Mini Batch KMeans算法的采样集的大小，默认是100.如果发现数据集的类别较多或者噪音点较多，需要增加这个值以达到较好的聚类效果。 
5）init： 即初始值选择的方式，和KMeans类的init意义一样。 
6）init_size: 用来做质心初始值候选的样本个数，默认是batch_size的3倍，一般用默认值就可以了。 
7）reassignment_ratio: 某个类别质心被重新赋值的最大次数比例，这个和max_iter一样是为了控制算法运行时间的。这个比例是占样本总数的比例，乘以样本总数就得到了每个类别质心可以重新赋值的次数。如果取值较高的话算法收敛时间可能会增加，尤其是那些暂时拥有样本数较少的质心。默认是0.01。如果数据量不是超大的话，比如1w以下，建议使用默认值。如果数据量超过1w，类别又比较多，可能需要适当减少这个比例值。具体要根据训练集来决定。 
8）max_no_improvement：即连续多少个Mini Batch没有改善聚类效果的话，就停止算法， 和reassignment_ratio， max_iter一样是为了控制算法运行时间的。默认是10.一般用默认值就足够了。

**K值的评估标准** 
不像监督学习的分类问题和回归问题，无监督聚类没有样本输出，也就没有比较直接的聚类评估方法。但是可以从簇内的稠密程度和簇间的离散程度来评估聚类的效果。常见的方法有轮廓系数Silhouette Coefficient和Calinski-Harabasz Index。个人比较喜欢Calinski-Harabasz Index，这个计算简单直接，得到的Calinski-Harabasz分数值s越大则聚类效果越好。



